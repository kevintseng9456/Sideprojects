{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Conv2D, Dense, DepthwiseConv2D, PReLU\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "\n",
    "from nets.arcface import arcface\n",
    "from nets.arcface_training import ArcFaceLoss, get_lr_scheduler\n",
    "from utils.callbacks import (ExponentDecayScheduler, LFW_callback, LossHistory,\n",
    "                             ParallelModelCheckpoint)\n",
    "from utils.dataloader import FacenetDataset, LFWDataset\n",
    "from utils.utils import get_acc, get_num_classes, show_config\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #---------------------------------------------------------------------#\n",
    "    #   train_gpu   訓練用到的GPU\n",
    "    #               默認為第一張卡、雙卡為[0, 1]、三卡為[0, 1, 2]\n",
    "    #               在使用多GPU時，每個卡上的batch為總batch除以卡的數量。\n",
    "    #---------------------------------------------------------------------#\n",
    "    train_gpu       = [0]\n",
    "    #--------------------------------------------------------#\n",
    "    #   指向根目錄下的cls_train.txt，讀取人臉路徑與標籤\n",
    "    #--------------------------------------------------------#\n",
    "    annotation_path = \"cls_train.txt\"\n",
    "    #--------------------------------------------------------#\n",
    "    #   輸入圖像大小，112*112為長寬，3為RGB圖 ，1為灰階圖\n",
    "    #--------------------------------------------------------#\n",
    "    input_shape     = [112, 112, 3]\n",
    "    #--------------------------------------------------------#\n",
    "    #   主幹特徵提取網絡的選擇\n",
    "    #   mobilefacenet\n",
    "    #   mobilenetv1\n",
    "    #   mobilenetv2\n",
    "    #   mobilenetv3\n",
    "    #   iresnet50\n",
    "    #\n",
    "    #   除了mobilenetv1外，其它的backbone均可從0開始訓練。\n",
    "    #--------------------------------------------------------#\n",
    "    backbone        = \"mobilefacenet\"\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    #   如果訓練過程中存在中斷訓練的操作，可以將model_path設置成logs文件夾下的權值文件，將已經訓練了一部分的權值再次載入。\n",
    "    #   同時修改下方的訓練的參數，來保證模型epoch的連續性。\n",
    "    #   \n",
    "    #   當model_path = ''的時候不加載整個模型的權值。\n",
    "    #\n",
    "    #   此處使用的是整個模型的權重，因此是在train.py進行加載的，pretrain不影響此處的權值加載。\n",
    "    #   如果想要讓模型從主幹的預訓練權值開始訓練，則設置model_path = 主幹的權值。\n",
    "    #   如果想要讓模型從0開始訓練，則設置model_path = ''，此時從0開始訓練。\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#  \n",
    "    model_path      = \"\"\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "   #   顯存不足與數據集大小無關，提示顯存不足請調小batch_size。\n",
    "    #   受到BatchNorm層影響，不能為1。\n",
    "    #\n",
    "    #   在此提供若干參數設置建議，各位訓練者根據自己的需求進行靈活調整：\n",
    "    #   （一）從預訓練權重開始訓練：\n",
    "    #       Adam：\n",
    "    #           Init_Epoch = 0，Epoch = 100，optimizer_type = 'adam'，Init_lr = 1e-3，weight_decay = 0。\n",
    "    #       SGD：\n",
    "    #           Init_Epoch = 0，Epoch = 100，optimizer_type = 'sgd'，Init_lr = 1e-2，weight_decay = 5e-4。\n",
    "    #       其中：UnFreeze_Epoch可以在100-300之間調整。\n",
    "    #   （二）batch_size的設置：\n",
    "    #       在顯卡能夠承受的範圍內，越大越好。顯存不足與訓練數據量無關，提示顯存不足(OOM或者CUDA out of memory）請調小請調小batch_size\n",
    "    #       受到BatchNorm層影響，batch_size最小為2，不能為1。\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    #------------------------------------------------------#\n",
    "    #   訓練參數\n",
    "    #   Init_Epoch      模型當前開始的訓練世代\n",
    "    #   Epoch           模型總共訓練的epoch(次數)\n",
    "    #   batch_size      每次輸入的圖片數量\n",
    "    #------------------------------------------------------#\n",
    "    Init_Epoch      = 0\n",
    "    Epoch           = 100\n",
    "    batch_size      = 12\n",
    "\n",
    "   #------------------------------------------------------------------#\n",
    "    #   其它訓練參數：學習率、優化器、學習率下降有關\n",
    "    #------------------------------------------------------------------#\n",
    "    #------------------------------------------------------------------#\n",
    "    #   Init_lr         模型的最大學習率\n",
    "    #   Min_lr          模型的最小學習率，默認為最大學習率的0.01\n",
    "    #------------------------------------------------------------------#\n",
    "    Init_lr             = 1e-2\n",
    "    Min_lr              = Init_lr * 0.01\n",
    "    #------------------------------------------------------------------#\n",
    "    #   optimizer_type  使用到的優化器種類，可選的有adam、sgd\n",
    "    #                   當使用Adam優化器時建議設置  Init_lr=1e-3\n",
    "    #                   當使用SGD優化器時建議設置   Init_lr=1e-2\n",
    "    #   momentum        優化器內部使用到的momentum參數\n",
    "    #   weight_decay    權值衰減，可防止過擬合\n",
    "    #                   adam會導致weight_decay錯誤，使用adam時建議設置為0。\n",
    "    #------------------------------------------------------------------#\n",
    "    optimizer_type      = \"sgd\"\n",
    "    momentum            = 0.9\n",
    "    weight_decay        = 5e-4\n",
    "    #------------------------------------------------------------------#\n",
    "    #   lr_decay_type   使用到的學習率下降方式，可選的有step、cos\n",
    "    #------------------------------------------------------------------#\n",
    "    lr_decay_type       = \"cos\"\n",
    "    #------------------------------------------------------------------#\n",
    "    #   save_period     多少個epoch保存一次權值，默認每個世代都保存\n",
    "    #------------------------------------------------------------------#\n",
    "    save_period         = 1\n",
    "    #------------------------------------------------------------------#\n",
    "    #   save_dir        權值與日誌文件保存的文件夾\n",
    "    #------------------------------------------------------------------#\n",
    "    save_dir            = 'logs'\n",
    "    #------------------------------------------------------------------#\n",
    "    #   用於設置是否使用多線程讀取數據\n",
    "    #   開啟後會加快數據讀取速度，但是會佔用更多內存\n",
    "    #   內存較小的電腦可以設置為2或者1\n",
    "    #------------------------------------------------------------------#\n",
    "    num_workers     = 1\n",
    "    #------------------------------------------------------------------#\n",
    "    #   是否开启LFW评估\n",
    "    #------------------------------------------------------------------#\n",
    "    lfw_eval_flag   = False\n",
    "    #------------------------------------------------------------------#\n",
    "    #   LFW評估數據集的文件路徑和對應的txt文件\n",
    "    #------------------------------------------------------------------#\n",
    "    lfw_dir_path    = \"lfw\"\n",
    "    lfw_pairs_path  = \"model_data/lfw_pair.txt\"\n",
    "\n",
    "    #------------------------------------------------------#\n",
    "    #   設置用到的顯卡\n",
    "    #------------------------------------------------------#\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]  = ','.join(str(x) for x in train_gpu)\n",
    "    ngpus_per_node                      = len(train_gpu)\n",
    "    print('Number of devices: {}'.format(ngpus_per_node))\n",
    "\n",
    "    num_classes = get_num_classes(annotation_path)\n",
    "    #-------------------------------------------#\n",
    "    #   建立模型\n",
    "    #-------------------------------------------#\n",
    "    model_body = arcface(input_shape, num_classes, backbone=backbone, mode=\"train\")\n",
    "    if model_path != '':\n",
    "        #------------------------------------------------------#\n",
    "        #   載入預訓練權重\n",
    "        #------------------------------------------------------#\n",
    "        print('Load weights {}.'.format(model_path))\n",
    "        model_body.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "        \n",
    "    if ngpus_per_node > 1:\n",
    "        #多GPU運算\n",
    "        model   = multi_gpu_model(model_body, gpus=ngpus_per_node)\n",
    "    else:\n",
    "        model   = model_body\n",
    "    #-------------------------------------------------------#\n",
    "    #   0.01用於驗證，0.99用於訓練\n",
    "    #-------------------------------------------------------#\n",
    "    val_split = 0.16\n",
    "    with open(annotation_path,\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "    \n",
    "    show_config(\n",
    "        num_classes = num_classes, backbone = backbone, model_path = model_path, input_shape = input_shape, \\\n",
    "        Init_Epoch = Init_Epoch, Epoch = Epoch, batch_size = batch_size, \\\n",
    "        Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
    "        save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
    "    )\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, DepthwiseConv2D):\n",
    "            layer.add_loss(l2(weight_decay)(layer.depthwise_kernel))\n",
    "        elif isinstance(layer, Conv2D) or isinstance(layer, Dense):\n",
    "            layer.add_loss(l2(weight_decay)(layer.kernel))\n",
    "        elif isinstance(layer, PReLU):\n",
    "            layer.add_loss(l2(weight_decay)(layer.alpha))\n",
    "\n",
    "    if True:\n",
    "        #-------------------------------------------------------------------#\n",
    "        #   判斷當前batch_size，自適應調整學習率\n",
    "        #-------------------------------------------------------------------#\n",
    "        nbs             = 64\n",
    "        lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 1e-1\n",
    "        lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "\n",
    "        optimizer = {\n",
    "            'adam'  : Adam(lr = Init_lr_fit, beta_1 = momentum),\n",
    "            'sgd'   : SGD(lr = Init_lr_fit, momentum = momentum, nesterov=True)\n",
    "        }[optimizer_type]\n",
    "        m = 0.5\n",
    "        s = 32 if backbone == \"mobilefacenet\" else 64\n",
    "        model.compile(optimizer = optimizer, loss={'ArcMargin': ArcFaceLoss(s = s, m = m)}, metrics={'ArcMargin': get_acc()})\n",
    "    \n",
    "        #---------------------------------------#\n",
    "        #   獲得學習率下降的公式\n",
    "        #---------------------------------------#\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, Epoch)\n",
    "\n",
    "        epoch_step          = num_train // batch_size\n",
    "        epoch_step_val      = num_val // batch_size\n",
    "\n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "\n",
    "        train_dataset   = FacenetDataset(input_shape, lines[:num_train], batch_size, num_classes, random = True)\n",
    "        val_dataset     = FacenetDataset(input_shape, lines[num_train:], batch_size, num_classes, random = False)\n",
    "\n",
    "        #-------------------------------------------------------------------------------#\n",
    "        #   訓練參數的設置\n",
    "        #   logging         用於設置tensorboard的保存地址\n",
    "        #   checkpoint      用於設置權值保存的細節，period用於修改多少epoch保存一次\n",
    "        #   lr_scheduler       用於設置學習率下降的方式\n",
    "        #   early_stopping  用於設定早停，val_loss多次不下降自動結束訓練，表示模型基本收斂\n",
    "        #-------------------------------------------------------------------------------#\n",
    "        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "        logging         = TensorBoard(log_dir)\n",
    "        loss_history    = LossHistory(log_dir)\n",
    "        if ngpus_per_node > 1:\n",
    "            #GPU保存模型 需調用ParallelModelCheckpoint解決code error問題\n",
    "            checkpoint      = ParallelModelCheckpoint(model_body, os.path.join(save_dir, \"ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\"), \n",
    "                                    monitor = 'val_loss', save_weights_only = True, save_best_only = False, period = save_period)\n",
    "        else:\n",
    "            checkpoint      = ModelCheckpoint(os.path.join(save_dir, \"ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\"), \n",
    "                                    monitor = 'val_loss', save_weights_only = True, save_best_only = False, period = save_period)\n",
    "        lr_scheduler    = LearningRateScheduler(lr_scheduler_func, verbose = 1)\n",
    "        #---------------------------------#\n",
    "        #   LFW估计\n",
    "        #---------------------------------#\n",
    "        if lfw_eval_flag:\n",
    "            lfw_callback    = LFW_callback(LFWDataset(dir=lfw_dir_path, pairs_path=lfw_pairs_path, batch_size=32, input_shape=input_shape))\n",
    "            callbacks       = [logging, loss_history, checkpoint, lr_scheduler, lfw_callback]\n",
    "        else:\n",
    "            callbacks       = [logging, loss_history, checkpoint, lr_scheduler]\n",
    "\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(\n",
    "            generator           = train_dataset,\n",
    "            steps_per_epoch     = epoch_step,\n",
    "            validation_data     = val_dataset,\n",
    "            validation_steps    = epoch_step_val,\n",
    "            epochs              = Epoch,\n",
    "            initial_epoch       = Init_Epoch,\n",
    "            use_multiprocessing = True if num_workers > 1 else False,\n",
    "            workers             = num_workers,\n",
    "            callbacks           = callbacks\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a50661e715ff64d60d8747fa7b1e3af6092d527f01cb31cb7daf74037322f8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
